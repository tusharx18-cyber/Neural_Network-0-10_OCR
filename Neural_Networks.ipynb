{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1bUvDZBnisj6","mYi99AacivWS","mncx2Xuyi0_g","Jce08PiYi-a5","Xtj1H1_MlVrq","eys3sg-rlh_5","la7GOauPln_w"],"authorship_tag":"ABX9TyMt3dhRLF/RkA7dRaqPV1iF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Neural Networks 0-1"],"metadata":{"id":"1bUvDZBnisj6"}},{"cell_type":"markdown","source":["\n","## Lets start with computing output of a single neuron. the formula goes as:\n","\n","$$ \\text{neuron output} = \\mathbf{input} \\cdot \\mathbf{weight} + bias $$\n"],"metadata":{"id":"mYi99AacivWS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zv3O0qXQhft9"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Now, lets compute multiple neurons at once, also known as a layer of neurons.\n","\n","$$ \\text{output} = \\sum_{i=0}^{n} (\\text{input}_i \\cdot \\text{weight}_i) + \\text{biases} $$\n","\n","Here input and weight are arrays of float values of which dot product is calculated."],"metadata":{"id":"mncx2Xuyi0_g"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"Kyon6P1Wi2yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gyP8tJMHi53R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ReLU Activation Function.\n","$$ \\text{ReLU}(x) = \\max(0, x) $$\n","\n","ReLU is mostly used to activate hidden layers."],"metadata":{"id":"Jce08PiYi-a5"}},{"cell_type":"code","source":[],"metadata":{"id":"95wJBiu1i-9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SoftMax Activation\n","\n","$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $$"],"metadata":{"id":"Xtj1H1_MlVrq"}},{"cell_type":"code","source":[],"metadata":{"id":"zsRBJ5B7lcTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loss Calculation (Categorical Cross-Entropy)\n","\n","$$ \\text{Loss} = -\\sum_{i} y_i \\log(p_i) $$"],"metadata":{"id":"eys3sg-rlh_5"}},{"cell_type":"code","source":[],"metadata":{"id":"e-iUXao6mVJ5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adam Optimizer\n","\n","### Compute the biased first moment estimate $m_t$\n","$$\n","m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n","$$\n","\n","- where $g_t$ is the gradient at time step $t$.\n","\n","### Compute the biased second raw moment estimate $v_t$:\n","\n","$$\n","v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2\n","$$\n","### Compute bias-corrected first moment estimate $\\hat{m}^t$:\n","\n","$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n","\n","### Compute bias-corrected second raw moment estimate $\\hat{v}^t$:\n","$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $$\n","\n","### Update the parameters $Î¸$:\n","$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha \\cdot \\hat{v}_t}{\\sqrt{\\hat{m}_t} + \\epsilon}$$"],"metadata":{"id":"la7GOauPln_w"}},{"cell_type":"code","source":[],"metadata":{"id":"Ii8Qjmw3lnKv"},"execution_count":null,"outputs":[]}]}